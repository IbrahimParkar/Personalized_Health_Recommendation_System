{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4aad47",
   "metadata": {},
   "source": [
    "# Heart Arrythmia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f05039",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "935b5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# Load your dataset from the input CSV file\n",
    "input_file = r'..Arrhythmia\\Data\\Seconds\\heartrate_seconds_merged.csv'\n",
    "output_file = r'..Arrhythmia\\Data\\Minutes\\minuteHeartrate.csv'\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Function to parse timestamps with multiple formats\n",
    "def parse_timestamp(timestamp):\n",
    "    try:\n",
    "        return parser.parse(timestamp)\n",
    "    except parser.ParserError:\n",
    "        # If the timestamp format is not recognized, return NaN (or handle as needed)\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply the parse_timestamp function to the \"Time\" column\n",
    "df['Time'] = df['Time'].apply(parse_timestamp)\n",
    "\n",
    "# Drop rows with NaN values in the \"Time\" column (optional)\n",
    "df = df.dropna(subset=['Time'])\n",
    "\n",
    "# Group by minute and calculate the average heart rate\n",
    "result = df.groupby(['Id', pd.Grouper(key='Time', freq='1Min')])['Value'].mean().reset_index()\n",
    "\n",
    "# Round off the 'Value' column to the nearest whole number\n",
    "result['Value'] = result['Value'].round().astype(int)\n",
    "\n",
    "# Save the result to the specified output CSV file\n",
    "result.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a4afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your heart rate dataset into a DataFrame (replace 'heart_rate.csv' with your actual file name)\n",
    "heart_rate_df = pd.read_csv(r'..Arrhythmia\\Data\\Minutes\\minuteheartrate.csv')\n",
    "\n",
    "# Standardize the date format in the 'Time' column using dateutil.parser\n",
    "heart_rate_df['Time'] = heart_rate_df['Time'].apply(lambda x: parse(x))\n",
    "\n",
    "# Load your minute sleep dataset into another DataFrame (replace 'minute_sleep.csv' with your actual file name)\n",
    "minute_sleep_df = pd.read_csv(r'..Arrhythmia\\Data\\Minutes\\minuteSleep.csv')\n",
    "\n",
    "# Standardize the date format in the 'Time' column in the minute sleep DataFrame\n",
    "minute_sleep_df['Time'] = minute_sleep_df['Time'].apply(lambda x: parse(x))\n",
    "\n",
    "# Create a new column 'Sleep' in the heart rate DataFrame and initialize it to 0\n",
    "heart_rate_df['Sleep'] = 0\n",
    "\n",
    "# Iterate through the heart rate DataFrame and check for matching 'Id' and 'Time' in the minute sleep DataFrame\n",
    "for index, row in heart_rate_df.iterrows():\n",
    "    match_condition = (minute_sleep_df['Id'] == row['Id']) & (minute_sleep_df['Time'] == row['Time'])\n",
    "    if match_condition.any():\n",
    "        heart_rate_df.at[index, 'Sleep'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f93eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column \"Abheartrate\" with default value 1\n",
    "heart_rate_df['Abheartrate'] = 1\n",
    "\n",
    "# Define the conditions and update the \"Abheartrate\" column accordingly\n",
    "# Condition 1: Heart rate between 60 and 100 and sleep is 0\n",
    "condition1 = (heart_rate_df['Value'] >= 60) & (heart_rate_df['Value'] <= 100) & (heart_rate_df['Sleep'] == 0)\n",
    "\n",
    "# Condition 2: Heart rate between 40 and 50 and sleep is 1\n",
    "condition2 = (heart_rate_df['Value'] >= 40) & (heart_rate_df['Value'] <= 50) & (heart_rate_df['Sleep'] == 1)\n",
    "\n",
    "# Set the \"Abheartrate\" column to 1 where either condition 1 or condition 2 is met\n",
    "heart_rate_df.loc[condition1 | condition2, 'Abheartrate'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01f59b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "# Read the original dataset from the CSV file\n",
    "df = pd.read_csv(r\"..Arrhythmia\\Intensities\\minuteIntensitiesWide_merged.csv\")\n",
    "\n",
    "# Initialize an empty list to store the transformed data\n",
    "transformed_data = []\n",
    "\n",
    "# Iterate through the rows of the filtered dataset\n",
    "for _, row in df.iterrows():\n",
    "    id_value = row['Id']\n",
    "    activity_hour = parse(row['ActivityHour'], fuzzy=True)\n",
    "    \n",
    "    for minute in range(60):\n",
    "        minute_str = f'Intensity{minute:02d}'\n",
    "        intensity_value = row[minute_str]\n",
    "        \n",
    "        # Create a new timestamp for each minute\n",
    "        minute_timestamp = activity_hour + pd.Timedelta(minutes=minute)\n",
    "        \n",
    "        # Append a row for each minute with the corresponding \"ActivityHour\" and \"Intensity\"\n",
    "        if intensity_value != 0:  # Only include rows with non-zero intensity\n",
    "            transformed_data.append([id_value, minute_timestamp, intensity_value])\n",
    "\n",
    "# Create a new DataFrame with the transformed data\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=['Id', 'Time', 'Intensity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "455e2970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first dataset\n",
    "df1 = transformed_df\n",
    "\n",
    "# Read the second dataset\n",
    "df2 = heart_rate_df\n",
    "\n",
    "# Merge the datasets based on both \"Id\" and \"Time\"\n",
    "merged_df = pd.merge(df2, df1, on=[\"Id\", \"Time\"], how=\"left\")\n",
    "\n",
    "# Reorder columns to place \"Intensity\" before \"Abheartrate\"\n",
    "merged_df = merged_df[[\"Id\", \"Time\", \"Value\", \"Sleep\", \"Intensity\", \"Abheartrate\"]]\n",
    "\n",
    "# Fill blanks in the \"Intensity\" column with zero\n",
    "merged_df[\"Intensity\"].fillna(0, inplace=True)\n",
    "\n",
    "# Make anything greater than zero in the \"Intensity\" column as 1\n",
    "merged_df[\"Intensity\"] = (merged_df[\"Intensity\"] > 0).astype(int)\n",
    "\n",
    "# Set \"Abheartrate\" values to 0 where \"Intensity\" is 1\n",
    "merged_df.loc[merged_df[\"Intensity\"] == 1, \"Abheartrate\"] = 0\n",
    "\n",
    "# Rename columns\n",
    "merged_df = merged_df.rename(columns={'Value': 'Bpm', 'Abheartrate': 'Arrhythmia'})\n",
    "\n",
    "# Save the merged dataset to a new CSV file\n",
    "merged_df.to_csv(r\"..Arrhythmia\\Data\\heartrate_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fa4c4b",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3dc751",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c51d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = pd.read_csv(r\"..Arrhythmia\\Data\\heartrate_final.csv\") \n",
    "\n",
    "# Prepare the features (X) and target variable (y)\n",
    "X = data[[\"Bpm\", \"Sleep\", \"Intensity\"]]\n",
    "y = data[\"Arrhythmia\"]\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa58907e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b741b",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c223069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrhythmia Model - Accuracy:  1.0\n",
      "Arrhythmia Model - Confusion Matrix:\n",
      " [[49541     0]\n",
      " [    0 17143]]\n",
      "Arrhythmia Model - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     49541\n",
      "           1       1.00      1.00      1.00     17143\n",
      "\n",
      "    accuracy                           1.00     66684\n",
      "   macro avg       1.00      1.00      1.00     66684\n",
      "weighted avg       1.00      1.00      1.00     66684\n",
      "\n",
      "Cross-Validation Scores: [1. 1. 1. 1. 1.]\n",
      "Mean CV Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r\"..Arrhythmia\\Data\\heartrate_final.csv\") \n",
    "\n",
    "# Drop the 'Time' and 'Id' column\n",
    "data = data.drop(columns=[\"Id\", 'Time'])\n",
    "\n",
    "# Prepare the features (X) and target variable (y)\n",
    "X = data[[\"Bpm\", \"Sleep\", \"Intensity\"]]\n",
    "y = data[\"Arrhythmia\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=30, random_state=42, class_weight = 'balanced')\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data for Arrhythmia\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model for Arrhythmia\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation results for Arrhythmia\n",
    "print(\"Arrhythmia Model - Accuracy: \", accuracy)\n",
    "print(\"Arrhythmia Model - Confusion Matrix:\\n\", confusion)\n",
    "print(\"Arrhythmia Model - Classification Report:\\n\", report)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(rf_classifier, X, y, cv=5)  # 5-fold cross-validation\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f6719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest\n",
      "Model - Accuracy: 1.0\n",
      "Model - Confusion Matrix:\n",
      " [[49541     0]\n",
      " [    0 17143]]\n",
      "Model - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     49541\n",
      "           1       1.00      1.00      1.00     17143\n",
      "\n",
      "    accuracy                           1.00     66684\n",
      "   macro avg       1.00      1.00      1.00     66684\n",
      "weighted avg       1.00      1.00      1.00     66684\n",
      "\n",
      "Cross-Validation Scores: [1. 1. 1. 1. 1.]\n",
      "Mean CV Accuracy: 1.0\n",
      "\n",
      "\n",
      "Training MLP\n",
      "Model - Accuracy: 1.0\n",
      "Model - Confusion Matrix:\n",
      " [[49541     0]\n",
      " [    0 17143]]\n",
      "Model - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     49541\n",
      "           1       1.00      1.00      1.00     17143\n",
      "\n",
      "    accuracy                           1.00     66684\n",
      "   macro avg       1.00      1.00      1.00     66684\n",
      "weighted avg       1.00      1.00      1.00     66684\n",
      "\n",
      "Cross-Validation Scores: [0.99742067 1.         0.96357447 1.         1.        ]\n",
      "Mean CV Accuracy: 0.9921990282526544\n",
      "\n",
      "\n",
      "Training SVM\n",
      "Model - Accuracy: 0.9388159078639554\n",
      "Model - Confusion Matrix:\n",
      " [[48387  1154]\n",
      " [ 2926 14217]]\n",
      "Model - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96     49541\n",
      "           1       0.92      0.83      0.87     17143\n",
      "\n",
      "    accuracy                           0.94     66684\n",
      "   macro avg       0.93      0.90      0.92     66684\n",
      "weighted avg       0.94      0.94      0.94     66684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r\"..Arrhythmia\\Data\\heartrate_final.csv\") \n",
    "\n",
    "# Drop the 'Time' and 'Id' column\n",
    "data = data.drop(columns=[\"Id\", 'Time'])\n",
    "\n",
    "# Prepare the features (X) and target variable (y)\n",
    "X = data[[\"Bpm\", \"Sleep\", \"Intensity\"]]\n",
    "y = data[\"Arrhythmia\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a dictionary of classifiers to compare\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n",
    "    \"SVM\": SVC(kernel='rbf', random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Iterate over each classifier\n",
    "for name, classifier in classifiers.items():\n",
    "    print(\"Training\", name)\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data for Arrhythmia\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model for Arrhythmia\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Print the evaluation results for Arrhythmia\n",
    "    print(\"Model - Accuracy:\", accuracy)\n",
    "    print(\"Model - Confusion Matrix:\\n\", confusion)\n",
    "    print(\"Model - Classification Report:\\n\", report)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(classifier, X, y, cv=5)  # 5-fold cross-validation\n",
    "    print(\"Cross-Validation Scores:\", cv_scores)\n",
    "    print(\"Mean CV Accuracy:\", np.mean(cv_scores))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d9952",
   "metadata": {},
   "source": [
    "100 % accuracy in random forest not useful for real world applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd2c9f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model - Accuracy:  0.8888488992861856\n",
      "Logistic Regression Model - Confusion Matrix:\n",
      " [[48504  1037]\n",
      " [ 6375 10768]]\n",
      "Logistic Regression Model - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93     49541\n",
      "           1       0.91      0.63      0.74     17143\n",
      "\n",
      "    accuracy                           0.89     66684\n",
      "   macro avg       0.90      0.80      0.84     66684\n",
      "weighted avg       0.89      0.89      0.88     66684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Algorithm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r\"..Arrhythmia\\Data\\heartrate_final.csv\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=[\"Id\", 'Time'])\n",
    "\n",
    "# Prepare the features (X) and target variable (y)\n",
    "X = data[[\"Bpm\", \"Sleep\", \"Intensity\"]]\n",
    "y = data[\"Arrhythmia\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a Logistic Regression model\n",
    "logreg_model = LogisticRegression(random_state=42)\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Logistic Regression Model - Accuracy: \", accuracy)\n",
    "print(\"Logistic Regression Model - Confusion Matrix:\\n\", confusion)\n",
    "print(\"Logistic Regression Model - Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b530e2b",
   "metadata": {},
   "source": [
    "### Gradient Descent With Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a59f6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
    "    m,n  = X.shape\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot\n",
    "        f_wb_i = sigmoid(z_i)                                          #scalar\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n",
    "             \n",
    "    cost = cost/m                                                      #scalar\n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          #scalar\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
    "   \n",
    "    total_cost = cost + reg_cost                                       #scalar\n",
    "    return total_cost                                                  #scalar\n",
    "\n",
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                            #(n,)\n",
    "    dj_db = 0.0                                       #scalar\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c451401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent function\n",
    "def gradient_descent_reg(X, y, w_in, b_in, alpha, num_iters,lambda_): \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic_reg(X, y, w, b,lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic_reg(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "# Convert DataFrame to numpy arrays for computation\n",
    "X_train_np = X_train.values\n",
    "y_train_np = y_train.values\n",
    "\n",
    "# Initialize parameters\n",
    "\n",
    "num_iters = 1000  # Number of iterations\n",
    "initial_w = np.zeros(X_train.shape[1])  # Initialize weights\n",
    "initial_b = 0  # Initialize bias\n",
    "\n",
    "alpha = 0.001  # Reduced learning rate\n",
    "lambda_tmp = 0.1  # Reduced regularization parameter\n",
    "\n",
    "# Apply gradient descent\n",
    "final_w, final_b, J_history = gradient_descent_reg(X_train_np, y_train_np, initial_w, initial_b, alpha, num_iters,lambda_tmp)\n",
    "\n",
    "# Print the final weights and bias\n",
    "print(\"Final weights:\", final_w)\n",
    "print(\"Final bias:\", final_b)\n",
    "\n",
    "# Evaluate the model on test data (For simplicity, we will only use the final weights and bias without iterating further)\n",
    "y_pred = (sigmoid(np.dot(X_test.values, final_w) + final_b) > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\nGradient Descent Logistic Regression Model - Accuracy: \", accuracy)\n",
    "print(\"Gradient Descent Logistic Regression Model - Confusion Matrix:\\n\", confusion)\n",
    "print(\"Gradient Descent Logistic Regression Model - Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f879d",
   "metadata": {},
   "source": [
    "### Gradient Descent Without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6774f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "        \n",
    "    return dj_db, dj_dw  \n",
    "\n",
    "def compute_cost_logistic(X, y, w, b):\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, w) + b)\n",
    "    cost = -1/m * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return cost\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7dee850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f326ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "# Initialize parameters\n",
    "alpha = 0.0001  # Learning rate\n",
    "num_iters = 1000  # Number of iterations\n",
    "initial_w = np.zeros(X_train.shape[1])  # Initialize weights\n",
    "initial_b = 0  # Initialize bias\n",
    "\n",
    "# Convert DataFrame to numpy arrays for computation\n",
    "X_train_np = X_train.values\n",
    "y_train_np = y_train.values\n",
    "\n",
    "# Apply gradient descent\n",
    "final_w, final_b, J_history = gradient_descent(X_train_np, y_train_np, initial_w, initial_b, alpha, num_iters)\n",
    "\n",
    "# Print the final weights and bias\n",
    "print(\"Final weights:\", final_w)\n",
    "print(\"Final bias:\", final_b)\n",
    "\n",
    "# Evaluate the model on test data (For simplicity, we will only use the final weights and bias without iterating further)\n",
    "y_pred = (sigmoid(np.dot(X_test.values, final_w) + final_b) > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"\\nGradient Descent Logistic Regression Model - Accuracy: \", accuracy)\n",
    "print(\"Gradient Descent Logistic Regression Model - Confusion Matrix:\\n\", confusion)\n",
    "print(\"Gradient Descent Logistic Regression Model - Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37284e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize logistic regression model\n",
    "logreg = LogisticRegression(max_iter=50000, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and other metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculate and print ROC AUC score\n",
    "y_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f'ROC AUC Score: {roc_auc:.2f}')\n",
    "\n",
    "# Define hyperparameters grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],  # Regularization type\n",
    "    'solver': ['liblinear', 'saga']  # Optimization algorithm\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,  # 5-fold cross-validation\n",
    "                           scoring='accuracy',\n",
    "                           verbose=1)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and accuracy\n",
    "print(f'Best Hyperparameters: {grid_search.best_params_}')\n",
    "print(f'Best Accuracy: {grid_search.best_score_ * 100:.2f}%')\n",
    "\n",
    "# Evaluate the model with best hyperparameters on the test set\n",
    "best_logreg = grid_search.best_estimator_\n",
    "y_pred_best = best_logreg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy and other metrics with best model\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f'Best Model Accuracy: {accuracy_best * 100:.2f}%')\n",
    "\n",
    "# Print classification report for best model\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Calculate and print ROC AUC score for best model\n",
    "y_prob_best = best_logreg.predict_proba(X_test)[:, 1]\n",
    "roc_auc_best = roc_auc_score(y_test, y_prob_best)\n",
    "print(f'Best Model ROC AUC Score: {roc_auc_best:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e2063",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0437786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have already loaded and preprocessed your data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a Logistic Regression model with L1 regularization (Lasso)\n",
    "# C is the inverse of regularization strength; smaller values specify stronger regularization\n",
    "logreg_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=1.0, random_state=42)\n",
    "logreg_l1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_l1 = logreg_l1.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_l1 = accuracy_score(y_test, y_pred_l1)\n",
    "print(\"Accuracy with L1 regularization:\", accuracy_l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d8f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a Logistic Regression model with L2 regularization (Ridge)\n",
    "# C is the inverse of regularization strength; smaller values specify stronger regularization\n",
    "logreg_l2 = LogisticRegression(penalty='l2', C=1.0, random_state=42)\n",
    "logreg_l2.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_l2 = logreg_l2.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n",
    "print(\"Accuracy with L2 regularization:\", accuracy_l2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c69bbe0",
   "metadata": {},
   "source": [
    "# Using Scikit learn (Best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34139835",
   "metadata": {},
   "source": [
    "### Using balanced class weight because the classes are imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc0787b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9351118709135625\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95     49568\n",
      "           1       0.81      0.97      0.88     17116\n",
      "\n",
      "    accuracy                           0.94     66684\n",
      "   macro avg       0.90      0.95      0.92     66684\n",
      "weighted avg       0.94      0.94      0.94     66684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45732  3836]\n",
      " [  491 16625]]\n",
      "Cross-Validation Scores: [0.97242217 0.96264471 0.880781   0.97024774 0.95150261]\n",
      "Mean CV Accuracy: 0.9475196448923281\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r\"..Arrhythmia\\Data\\heartrate_final.csv\")\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=[\"Id\", 'Time'])\n",
    "# Prepare the features (X) and target variable (y)\n",
    "X = data[[\"Bpm\", \"Sleep\", \"Intensity\"]]\n",
    "y = data[\"Arrhythmia\"]\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# Initialize and train the logistic regression model\n",
    "lr_model = LogisticRegression(class_weight='balanced') \n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(lr_model, X, y, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score:\", accuracy)\n",
    "# Generate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "# Generate confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a42d3dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the full path to save the pickle file\n",
    "file_path = r'..Webpage\\model\\Ha_lr.pickle'\n",
    "\n",
    "# Save the model to a pickle file\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca22686-2a86-4d95-8bb6-22a5a91bfad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for the new data: [1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with the new data\n",
    "new_data = pd.DataFrame({\"Bpm\": [58], \"Sleep\": [0], \"Intensity\": [0]})\n",
    "\n",
    "# Make predictions using the trained logistic regression model\n",
    "predictions = lr_model.predict(new_data)\n",
    "\n",
    "# Display the predictions\n",
    "print(\"Predicted class for the new data:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
